{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LrqqcTKbmXNz",
        "vI9xsIRfmckl",
        "PpQfoFzysCTu"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Conversation Part"
      ],
      "metadata": {
        "id": "LrqqcTKbmXNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "from IPython import display\n",
        "display.clear_output()"
      ],
      "metadata": {
        "id": "6CpUpXIkldx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"sk-proj-z45joqHw9hFL8ighuOHoT3BlbkFJ5hdiMWnvs3sfROsHzBhk\""
      ],
      "metadata": {
        "id": "35a6VehPllg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=api_key,)"
      ],
      "metadata": {
        "id": "T18X1x5elpz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_assistant_response(messages):\n",
        "    r = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in messages],\n",
        "    )\n",
        "    response = r.choices[0].message.content\n",
        "    return response"
      ],
      "metadata": {
        "id": "1jTLsFwTlyCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "def display_chat_history2(messages):\n",
        "    for key, val in messages.items():\n",
        "        print(f\"{key}: {val}\")"
      ],
      "metadata": {
        "id": "O-qiEnn8l0AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"You are a respectful and helpful therapist, here to support patients and listen carefully to their concerns and feelings using your knowledge.\n",
        "       Your responses should always be in colloquia saudi hijazi Arabic.\n",
        "        You are the user's friend. so please speak like a helpful friend.\n",
        "        Ensure that your responses are unbiased and positive in nature.\n",
        "        If a question does not make sense or is not factually coherent, explain why instead of providing an incorrect answer.\n",
        "        If you don't know the answer to a question, do not share false information.\n",
        "        Make sure you make the user in the most comfortable space to speak and engage.\n",
        "        Speak with love and empathy.\n",
        "        Your goal is to provide psychological support, help patients deal with their emotions and concerns in a respectful and supportive manner, and act as a therapist during the conversation.\n",
        "        Engage with patients in a conversational manner, asking questions to understand their symptoms and feelings better, and provide guidance and support as a therapist would.\n",
        "        Embed Hamilton rating scale within stories and conversations and diagnose the patient based on their answers and provide the score to the user if the user finishes the conversation\"\"\"\n",
        "\n",
        "messages = [{'role': 'system', 'content':prompt }]"
      ],
      "metadata": {
        "id": "kOMNMGQSl3H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 2\n",
        "while True:\n",
        "\n",
        "    user_input = input(\"User: \")\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    assistant_response = get_assistant_response(messages)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "    display_chat_history2(messages[i])\n",
        "    i += 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "i4pvpay9l7io",
        "outputId": "da39df09-da47-494c-8648-881b7a64978d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: احس بكتمة اليوم\n",
            "role: assistant\n",
            "content: الله يكون في عونك، ممكن تقول لي أكثر عن اللي مضايقك؟ عطيني تفاصيل أكثر علشان نقدر نفهم سوا شنو اللي صاير ونعرف كيف نساعدك. كيف حالتك النفسية الفترة الأخيرة؟ هل صار معاك شيء مسبب لك هالكتمة؟\n",
            "User: معرف احس مالي نفس\n",
            "role: assistant\n",
            "content: حبيبي، أحس بمعاناتك. مرات نمر بأيام نحس فيها بثقل على قلوبنا ونحس إن مافي شيء يعطينا نفس من الحياة. ممكن نسوي شيء بسيط عشان تساعدنا نفهم حالتك أكثر؟ خليني أسألك شوية أسئلة بسيطة. \n",
            "\n",
            "1. كيف نومك الفترة الأخيرة؟ هل تحس إنك تنام كفاية ولا عندك صعوبة في النوم؟\n",
            "2. شهيتك للأكل كيف صارت؟ تاكل كالمعتاد ولا تحس إنك فاقد الرغبة في الأكل؟\n",
            "3. كيف تقضي وقتك يومياً؟ هل تشعر بمتعة في الأنشطة اللي كنت تحبها؟\n",
            "\n",
            "هالأسئلة ممكن تساعدنا نفهم حالتك بشكل أفضل ونقدر نحدد إذا كان الأفضل تحتاج مساعدة إضافية. أنا هنا اسمعك وأساعدك بجميع الطرق الممكنة.\n",
            "User: ما انام كفاية\n",
            "role: assistant\n",
            "content: الله يعينك، قلة النوم فعلاً تؤثر على النفسية بشكل كبير. لما ما ننام منيح، تصير أجسامنا وعقولنا مجهدة، وهذا يزيد من الإحساس بالكتمة والتعب. \n",
            "\n",
            "بدي أسألك: \n",
            "1. كم ساعة تقريباً تنام في اليوم؟\n",
            "2. هل تصير عندك صعوبة في النوم من البداية ولا تقوم من النوم وتلقي صعوبة في الرجوع للنوم؟\n",
            "3. فيه أشياء أو أفكار معينة تشغل بالك لما تحاول تنام؟\n",
            "\n",
            "خلينا نحاول نفهم أكثر عشان نقدر نساعدك بطريقة أفضل. إذا عندك أي شيء ثاني تبي تقوله أو تفضفض فيه، أنا هنا أسمعك وداعماً لك.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9295b0747119>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classifier Part"
      ],
      "metadata": {
        "id": "vI9xsIRfmckl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = []\n",
        "for msg in messages:\n",
        "  if msg['role'] == 'user':\n",
        "    user_input.append(msg['content'])"
      ],
      "metadata": {
        "id": "-m-oJCkom8q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQt-jVmqmbyi",
        "outputId": "680061b6-7a02-495e-cb41-b3a46e468177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_path = '/content/gdrive/MyDrive/MHD_Project/PHQ9_Classifier'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "xxSL_YTamunR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping predictions to classes\n",
        "label_mapping = {0: 'Diminished ability to think or concentrate', 1: 'Feelings of worthlessness', 2: 'Psychomotor agitation or retardation', 3: 'Suicidality', 4: 'losing interest or pleasure in activities', 5: 'loss of energy', 6: 'low mood', 7: 'sleep disorder', 8: 'weight disorder'}"
      ],
      "metadata": {
        "id": "VWl_uYXBn2jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chaUGxSlmw5i",
        "outputId": "d80959a1-2359-43d3-e29f-9d615fd31bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_predictions(predictions, label_mapping):\n",
        "    mapped_predictions = []\n",
        "    for prediction in predictions:\n",
        "        mapped_prediction = {}\n",
        "        for pred in prediction:\n",
        "            label = pred['label']\n",
        "            score = pred['score']\n",
        "            actual_label = label_mapping[int(label.split('_')[-1])]\n",
        "            mapped_prediction[actual_label] = score\n",
        "\n",
        "        mapped_predictions.append(mapped_prediction)\n",
        "    mapped_predictions = sorted(mapped_predictions[0].items(), key=lambda x: x[1], reverse=True)\n",
        "    return mapped_predictions\n",
        "\n",
        "def printMappedPreds(mapped_predictions):\n",
        "  for key, value in mapped_predictions:\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "\n",
        "for txt in user_input:\n",
        "  print(txt)\n",
        "  pred = pipeline(txt)\n",
        "  mapped_predictions = map_predictions(pred, label_mapping)\n",
        "  printMappedPreds(mapped_predictions)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg1T86SGoNRF",
        "outputId": "69a4a1fa-8d18-4de0-b270-e436089745dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "احس بكتمة اليوم\n",
            "low mood: 0.9989181756973267\n",
            "losing interest or pleasure in activities: 0.00019326248730067164\n",
            "Suicidality: 0.00017561156710144132\n",
            "loss of energy: 0.00017288699746131897\n",
            "sleep disorder: 0.00016302293806802481\n",
            "Diminished ability to think or concentrate: 0.000133487832499668\n",
            "Psychomotor agitation or retardation: 0.00011066508886869997\n",
            "weight disorder: 8.0635589256417e-05\n",
            "Feelings of worthlessness: 5.2224706450942904e-05\n",
            "\n",
            "معرف احس مالي نفس\n",
            "low mood: 0.9906747937202454\n",
            "losing interest or pleasure in activities: 0.004049259703606367\n",
            "loss of energy: 0.0011199024738743901\n",
            "weight disorder: 0.0009971152758225799\n",
            "Suicidality: 0.0008982856525108218\n",
            "sleep disorder: 0.0008738125907257199\n",
            "Diminished ability to think or concentrate: 0.000603980734013021\n",
            "Psychomotor agitation or retardation: 0.00046908666263334453\n",
            "Feelings of worthlessness: 0.0003137890307698399\n",
            "\n",
            "ما انام كفاية\n",
            "sleep disorder: 0.9990108013153076\n",
            "low mood: 0.000177136724232696\n",
            "losing interest or pleasure in activities: 0.00016399804735556245\n",
            "loss of energy: 0.00015122018521651626\n",
            "weight disorder: 0.00012779161625076085\n",
            "Suicidality: 0.00010333949467167258\n",
            "Diminished ability to think or concentrate: 9.538237645756453e-05\n",
            "Feelings of worthlessness: 8.836919732857496e-05\n",
            "Psychomotor agitation or retardation: 8.191512461053208e-05\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Voice Part"
      ],
      "metadata": {
        "id": "PpQfoFzysCTu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pUYG3Ya9s0AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a= !whisper \"/content/ElevenLabs_2024-07-31T11_02_36_Rachel_pre_s50_sb75_se0_b_m2.mp3\" --model medium --language Arabic"
      ],
      "metadata": {
        "id": "lAkjozQWsGA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqSiuNBH0tn6",
        "outputId": "3a6099c3-9b3d-4157-e12e-7907414dec6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-ya8h23c3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-ya8h23c3\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.3.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=908400b5d2b1c2defbe1c0eda7043504de01e8e8a5c883a997dde395f6df069f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hapumv4x/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXW6fwWg0rl5",
        "outputId": "0eaf3367-f67c-4a7e-cff3-22ef658c3418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting whisper\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from whisper) (1.16.0)\n",
            "Building wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=b198231d572eb824f739b79cd795b5b091336cbc2e950811c81d8f51908a2771\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/7c/1d/015619716e2facae6631312503baf3c3220e6a9a3508cb14b6\n",
            "Successfully built whisper\n",
            "Installing collected packages: whisper\n",
            "Successfully installed whisper-1.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6mL8f6P0u70",
        "outputId": "582412d0-4a1c-48bf-d3a2-84c8ca06ae2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to security.ub\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to security.ub\u001b[0m\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [861 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,217 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,396 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,421 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,884 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,115 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,552 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,785 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,130 kB]\n",
            "Fetched 24.8 MB in 3s (8,183 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "62 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 62 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "594fb_BN0v3V",
        "outputId": "57afa703-b6d9-4369-c7a1-2bd49a22fa51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20231117)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.3.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.7.0)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J6SWWNf0wrf",
        "outputId": "67b1dc67-9d08-42ef-b7fc-0d518aaf4ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: whisper [-h] [--model MODEL] [--model_dir MODEL_DIR] [--device DEVICE]\n",
            "               [--output_dir OUTPUT_DIR] [--output_format {txt,vtt,srt,tsv,json,all}]\n",
            "               [--verbose VERBOSE] [--task {transcribe,translate}]\n",
            "               [--language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}]\n",
            "               [--temperature TEMPERATURE] [--best_of BEST_OF] [--beam_size BEAM_SIZE]\n",
            "               [--patience PATIENCE] [--length_penalty LENGTH_PENALTY]\n",
            "               [--suppress_tokens SUPPRESS_TOKENS] [--initial_prompt INITIAL_PROMPT]\n",
            "               [--condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT] [--fp16 FP16]\n",
            "               [--temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK]\n",
            "               [--compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD]\n",
            "               [--logprob_threshold LOGPROB_THRESHOLD] [--no_speech_threshold NO_SPEECH_THRESHOLD]\n",
            "               [--word_timestamps WORD_TIMESTAMPS] [--prepend_punctuations PREPEND_PUNCTUATIONS]\n",
            "               [--append_punctuations APPEND_PUNCTUATIONS] [--highlight_words HIGHLIGHT_WORDS]\n",
            "               [--max_line_width MAX_LINE_WIDTH] [--max_line_count MAX_LINE_COUNT]\n",
            "               [--max_words_per_line MAX_WORDS_PER_LINE] [--threads THREADS]\n",
            "               [--clip_timestamps CLIP_TIMESTAMPS]\n",
            "               [--hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD]\n",
            "               audio [audio ...]\n",
            "\n",
            "positional arguments:\n",
            "  audio                 audio file(s) to transcribe\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model MODEL         name of the Whisper model to use (default: small)\n",
            "  --model_dir MODEL_DIR\n",
            "                        the path to save model files; uses ~/.cache/whisper by default (default:\n",
            "                        None)\n",
            "  --device DEVICE       device to use for PyTorch inference (default: cuda)\n",
            "  --output_dir OUTPUT_DIR, -o OUTPUT_DIR\n",
            "                        directory to save the outputs (default: .)\n",
            "  --output_format {txt,vtt,srt,tsv,json,all}, -f {txt,vtt,srt,tsv,json,all}\n",
            "                        format of the output file; if not specified, all available formats will be\n",
            "                        produced (default: all)\n",
            "  --verbose VERBOSE     whether to print out the progress and debug messages (default: True)\n",
            "  --task {transcribe,translate}\n",
            "                        whether to perform X->X speech recognition ('transcribe') or X->English\n",
            "                        translation ('translate') (default: transcribe)\n",
            "  --language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}\n",
            "                        language spoken in the audio, specify None to perform language detection\n",
            "                        (default: None)\n",
            "  --temperature TEMPERATURE\n",
            "                        temperature to use for sampling (default: 0)\n",
            "  --best_of BEST_OF     number of candidates when sampling with non-zero temperature (default: 5)\n",
            "  --beam_size BEAM_SIZE\n",
            "                        number of beams in beam search, only applicable when temperature is zero\n",
            "                        (default: 5)\n",
            "  --patience PATIENCE   optional patience value to use in beam decoding, as in\n",
            "                        https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to\n",
            "                        conventional beam search (default: None)\n",
            "  --length_penalty LENGTH_PENALTY\n",
            "                        optional token length penalty coefficient (alpha) as in\n",
            "                        https://arxiv.org/abs/1609.08144, uses simple length normalization by\n",
            "                        default (default: None)\n",
            "  --suppress_tokens SUPPRESS_TOKENS\n",
            "                        comma-separated list of token ids to suppress during sampling; '-1' will\n",
            "                        suppress most special characters except common punctuations (default: -1)\n",
            "  --initial_prompt INITIAL_PROMPT\n",
            "                        optional text to provide as a prompt for the first window. (default: None)\n",
            "  --condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT\n",
            "                        if True, provide the previous output of the model as a prompt for the next\n",
            "                        window; disabling may make the text inconsistent across windows, but the\n",
            "                        model becomes less prone to getting stuck in a failure loop (default:\n",
            "                        True)\n",
            "  --fp16 FP16           whether to perform inference in fp16; True by default (default: True)\n",
            "  --temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK\n",
            "                        temperature to increase when falling back when the decoding fails to meet\n",
            "                        either of the thresholds below (default: 0.2)\n",
            "  --compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD\n",
            "                        if the gzip compression ratio is higher than this value, treat the\n",
            "                        decoding as failed (default: 2.4)\n",
            "  --logprob_threshold LOGPROB_THRESHOLD\n",
            "                        if the average log probability is lower than this value, treat the\n",
            "                        decoding as failed (default: -1.0)\n",
            "  --no_speech_threshold NO_SPEECH_THRESHOLD\n",
            "                        if the probability of the <|nospeech|> token is higher than this value AND\n",
            "                        the decoding has failed due to `logprob_threshold`, consider the segment\n",
            "                        as silence (default: 0.6)\n",
            "  --word_timestamps WORD_TIMESTAMPS\n",
            "                        (experimental) extract word-level timestamps and refine the results based\n",
            "                        on them (default: False)\n",
            "  --prepend_punctuations PREPEND_PUNCTUATIONS\n",
            "                        if word_timestamps is True, merge these punctuation symbols with the next\n",
            "                        word (default: \"'“¿([{-)\n",
            "  --append_punctuations APPEND_PUNCTUATIONS\n",
            "                        if word_timestamps is True, merge these punctuation symbols with the\n",
            "                        previous word (default: \"'.。,，!！?？:：”)]}、)\n",
            "  --highlight_words HIGHLIGHT_WORDS\n",
            "                        (requires --word_timestamps True) underline each word as it is spoken in\n",
            "                        srt and vtt (default: False)\n",
            "  --max_line_width MAX_LINE_WIDTH\n",
            "                        (requires --word_timestamps True) the maximum number of characters in a\n",
            "                        line before breaking the line (default: None)\n",
            "  --max_line_count MAX_LINE_COUNT\n",
            "                        (requires --word_timestamps True) the maximum number of lines in a segment\n",
            "                        (default: None)\n",
            "  --max_words_per_line MAX_WORDS_PER_LINE\n",
            "                        (requires --word_timestamps True, no effect with --max_line_width) the\n",
            "                        maximum number of words in a segment (default: None)\n",
            "  --threads THREADS     number of threads used by torch for CPU inference; supercedes\n",
            "                        MKL_NUM_THREADS/OMP_NUM_THREADS (default: 0)\n",
            "  --clip_timestamps CLIP_TIMESTAMPS\n",
            "                        comma-separated list start,end,start,end,... timestamps (in seconds) of\n",
            "                        clips to process, where the last end timestamp defaults to the end of the\n",
            "                        file (default: 0)\n",
            "  --hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD\n",
            "                        (requires --word_timestamps True) skip silent periods longer than this\n",
            "                        threshold (in seconds) when a possible hallucination is detected (default:\n",
            "                        None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a= !whisper \"/content/ElevenLabs_2024-07-31T11_02_36_Rachel_pre_s50_sb75_se0_b_m2.mp3\" --model medium --language Arabic"
      ],
      "metadata": {
        "id": "pYC11VrUupmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"medium\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9jSxlgxsg2K",
        "outputId": "7d38a77b-f056-4e10-bb41-39ba47858212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:19<00:00, 80.0MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result = model.transcribe('/content/gdrive/MyDrive/MHD_Project/myTestAudio.mp3', language='Arabic')\n",
        "result = result['text']\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH0U_hCTss_m",
        "outputId": "b536b46f-0839-42f1-b87a-e010e0e422e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " أحس أني متضايق مادري وش الحل\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = pipeline(result)\n",
        "mapped_predictions = map_predictions(pred, label_mapping)\n",
        "printMappedPreds(mapped_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrie6a8W1mcg",
        "outputId": "44301879-8efb-45c4-d081-d2061b34fd92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "low mood: 0.9991075396537781\n",
            "losing interest or pleasure in activities: 0.00020606769248843193\n",
            "sleep disorder: 0.0001547923166071996\n",
            "Diminished ability to think or concentrate: 0.0001147159346146509\n",
            "Suicidality: 0.0001027672624331899\n",
            "loss of energy: 0.00010202189878327772\n",
            "Psychomotor agitation or retardation: 9.308212611358613e-05\n",
            "weight disorder: 7.435386214638129e-05\n",
            "Feelings of worthlessness: 4.456700480659492e-05\n"
          ]
        }
      ]
    }
  ]
}